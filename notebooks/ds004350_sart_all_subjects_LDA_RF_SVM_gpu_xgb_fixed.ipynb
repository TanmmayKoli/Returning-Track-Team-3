{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65823104",
   "metadata": {},
   "source": [
    "# ds004350 (OpenNeuro) â€” SART Sustained Attention: All-Subjects Training + LDA / RF / SVM\n",
    "\n",
    "This notebook will:\n",
    "- Discover **all subjects/sessions** with SART files under `data/openneuro/ds004350_subset/`\n",
    "- Load EEG from **EEGLAB `.set`** using `pymatreader` (avoids MNE EEGLAB event parsing issues)\n",
    "- Read `*_events.tsv` and create **lapse labels** (commission errors on NO-GO)\n",
    "- Preprocess EEG (avg ref, 1â€“40 Hz)\n",
    "- Epoch trials and extract **bandpower features** (theta/alpha/beta per channel)\n",
    "- Train and evaluate **LDA**, **Random Forest**, and **SVM** under:\n",
    "  1) **Cross-session (within-subject):** train `ses-pre` â†’ test `ses-post`\n",
    "  2) **Leave-One-Subject-Out (LOSO):** group generalization across subjects\n",
    "\n",
    "> Notes:\n",
    "> - Lapse events are typically rare (heavy class imbalance). We report **balanced accuracy**, **ROC-AUC**, and **lapse recall/precision**.\n",
    "> - If your `events.tsv` schema differs, adjust `label_sart_lapses()` once after inspecting a few files.\n",
    "\n",
    "## Expected folder\n",
    "`Neurotech_Model/data/openneuro/ds004350_subset/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11f23a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: pandas in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: scipy in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (1.8.0)\n",
      "Requirement already satisfied: mne in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (1.11.0)\n",
      "Requirement already satisfied: pymatreader in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (3.10.8)\n",
      "Requirement already satisfied: pyarrow in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (23.0.0)\n",
      "Requirement already satisfied: xgboost in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (3.1.3)\n",
      "Requirement already satisfied: joblib in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: decorator in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from mne) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from mne) (3.1.6)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: packaging in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from mne) (25.0)\n",
      "Requirement already satisfied: pooch>=1.5 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: h5py in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from pymatreader) (3.15.1)\n",
      "Requirement already satisfied: xmltodict in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from pymatreader) (1.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from pooch>=1.5->mne) (4.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from pooch>=1.5->mne) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2026.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tanmmay/Coding_projects/Neurotech_Model/.venv/lib/python3.11/site-packages (from jinja2->mne) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U numpy pandas scipy scikit-learn mne pymatreader tqdm matplotlib pyarrow xgboost joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4df19338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNE: 1.11.0\n",
      "pyarrow: 23.0.0\n",
      "xgboost: 3.1.3\n"
     ]
    }
   ],
   "source": [
    "# If needed (fresh env), uncomment and run:\n",
    "# \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from pymatreader import read_mat\n",
    "\n",
    "mne.set_log_level(\"WARNING\")\n",
    "print(\"MNE:\", mne.__version__)\n",
    "\n",
    "\n",
    "# Optional (for parquet cache + GPU boosting)\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    print(\"pyarrow:\", pa.__version__)\n",
    "except Exception as e:\n",
    "    print(\"pyarrow not available (parquet saving may fall back):\", repr(e))\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"xgboost:\", xgb.__version__)\n",
    "except Exception as e:\n",
    "    print(\"xgboost not available (GPU boosting disabled):\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc28517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SART_ROOT: /Users/tanmmay/Coding_projects/Neurotech_Model/data/openneuro/ds004350_subset exists: True\n",
      "Found SART runs (events.tsv): 48\n",
      " - sub-001/ses-post/eeg/sub-001_ses-post_task-SART_events.tsv\n",
      " - sub-001/ses-pre/eeg/sub-001_ses-pre_task-SART_events.tsv\n",
      " - sub-002/ses-post/eeg/sub-002_ses-post_task-SART_events.tsv\n",
      " - sub-002/ses-pre/eeg/sub-002_ses-pre_task-SART_events.tsv\n",
      " - sub-003/ses-post/eeg/sub-003_ses-post_task-SART_events.tsv\n",
      " - sub-003/ses-pre/eeg/sub-003_ses-pre_task-SART_events.tsv\n",
      " - sub-004/ses-post/eeg/sub-004_ses-post_task-SART_events.tsv\n",
      " - sub-004/ses-pre/eeg/sub-004_ses-pre_task-SART_events.tsv\n"
     ]
    }
   ],
   "source": [
    "# ---- Paths ----\n",
    "# Run from Neurotech_Model repo root (so relative path works)\n",
    "SART_ROOT = Path(\"../data/openneuro/ds004350_subset\").resolve()\n",
    "print(\"SART_ROOT:\", SART_ROOT, \"exists:\", SART_ROOT.exists())\n",
    "\n",
    "run_events = sorted(SART_ROOT.glob(\"sub-*/ses-*/eeg/*task-SART*_events.tsv\"))\n",
    "print(\"Found SART runs (events.tsv):\", len(run_events))\n",
    "for p in run_events[:8]:\n",
    "    print(\" -\", p.relative_to(SART_ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8762c",
   "metadata": {},
   "source": [
    "## 1) Helpers: parse IDs, label lapses, load EEG from `.set` (robust)\n",
    "\n",
    "We load EEG data from `.set` via `pymatreader` and build an `mne.RawArray`.\n",
    "We **do not** use EEGLAB events from the `.set` (we rely on `events.tsv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9071a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_subject_session(events_path: Path) -> tuple[str, str]:\n",
    "    sub = events_path.parts[-4]  # sub-001\n",
    "    ses = events_path.parts[-3]  # ses-pre\n",
    "    return sub, ses\n",
    "\n",
    "def read_events_tsv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    if \"onset\" not in df.columns:\n",
    "        raise KeyError(f\"'onset' missing in {path.name}. Columns={df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def label_sart_lapses(df_events: pd.DataFrame, no_go_digit: int = 3) -> pd.DataFrame:\n",
    "    df = df_events.copy()\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # ---------- 1) Identify stimulus digit / NO-GO ----------\n",
    "    is_nogo = None\n",
    "\n",
    "    # If trial_type exists, try to extract a digit or \"nogo\"\n",
    "    if \"trial_type\" in cols:\n",
    "        tt = df[cols[\"trial_type\"]].astype(str).str.lower()\n",
    "        digit = pd.to_numeric(tt.str.extract(r\"([0-9])\")[0], errors=\"coerce\")\n",
    "        is_nogo = (tt.str.contains(\"nogo\") | tt.str.contains(\"no-go\") | (digit == no_go_digit))\n",
    "    elif \"stimulus\" in cols:\n",
    "        stim = df[cols[\"stimulus\"]].astype(str).str.lower()\n",
    "        digit = pd.to_numeric(stim.str.extract(r\"([0-9])\")[0], errors=\"coerce\")\n",
    "        is_nogo = (stim.str.contains(\"nogo\") | (digit == no_go_digit))\n",
    "    elif \"value\" in cols:\n",
    "        val = df[cols[\"value\"]].astype(str).str.lower()\n",
    "        digit = pd.to_numeric(val.str.extract(r\"([0-9])\")[0], errors=\"coerce\")\n",
    "        is_nogo = (val.str.contains(\"nogo\") | (digit == no_go_digit))\n",
    "    else:\n",
    "        raise KeyError(f\"Can't find trial_type/stimulus/value in events columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # ---------- 2) Determine whether participant responded ----------\n",
    "    rt_col = None\n",
    "    for k in [\"response_time\", \"reaction_time\", \"rt\"]:\n",
    "        if k in cols:\n",
    "            rt_col = cols[k]\n",
    "            break\n",
    "\n",
    "    # Prefer explicit response column if present (more reliable than RT)\n",
    "    resp_col = None\n",
    "    for k in [\"response\", \"key\", \"button\", \"pressed\"]:\n",
    "        if k in cols:\n",
    "            resp_col = cols[k]\n",
    "            break\n",
    "\n",
    "    if resp_col is not None:\n",
    "        s = df[resp_col]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            responded = s.notna() & (s != 0)\n",
    "        else:\n",
    "            ss = s.astype(str).str.lower()\n",
    "            responded = ~ss.isin([\"\", \"nan\", \"none\", \"n/a\", \"na\", \"0\", \"false\"])\n",
    "    elif rt_col is not None:\n",
    "        rt = pd.to_numeric(df[rt_col], errors=\"coerce\")\n",
    "        responded = rt.notna()\n",
    "    else:\n",
    "        # If no response info exists, fall back to correctness if available\n",
    "        responded = None\n",
    "\n",
    "    # ---------- 3) If correct/accuracy exists, use it robustly ----------\n",
    "    correct = None\n",
    "    if \"correct\" in cols:\n",
    "        s = df[cols[\"correct\"]]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            correct = (s.fillna(1) != 0)\n",
    "        else:\n",
    "            ss = s.astype(str).str.lower()\n",
    "            correct = ss.isin([\"1\", \"true\", \"correct\", \"yes\"])\n",
    "    elif \"accuracy\" in cols:\n",
    "        s = df[cols[\"accuracy\"]]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            correct = (s.fillna(1) != 0)\n",
    "        else:\n",
    "            ss = s.astype(str).str.lower()\n",
    "            correct = ss.isin([\"1\", \"true\", \"correct\", \"yes\"])\n",
    "\n",
    "    # ---------- 4) Define lapses ----------\n",
    "    # SART lapse = commission (respond on no-go) OR omission (no response on go)\n",
    "    if responded is not None:\n",
    "        commission = is_nogo & responded\n",
    "        omission   = (~is_nogo) & (~responded)\n",
    "        lapse = (commission | omission).astype(int)\n",
    "    elif correct is not None:\n",
    "        # fallback: any incorrect is a lapse\n",
    "        lapse = (~correct).astype(int)\n",
    "    else:\n",
    "        raise KeyError(f\"No response/RT and no correct/accuracy columns found: {df.columns.tolist()}\")\n",
    "\n",
    "    df[\"is_nogo\"] = is_nogo.astype(int)\n",
    "    df[\"y\"] = lapse.astype(int)      # <-- use y consistently\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_eeg_from_set(events_tsv: Path) -> mne.io.RawArray:\n",
    "    base = events_tsv.name.replace(\"_events.tsv\", \"\")\n",
    "    set_path = events_tsv.with_name(base + \"_eeg.set\")\n",
    "    ch_tsv   = events_tsv.with_name(base + \"_channels.tsv\")\n",
    "    if not set_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {set_path}\")\n",
    "\n",
    "    mat = read_mat(str(set_path))\n",
    "    EEG = mat.get(\"EEG\", mat)\n",
    "\n",
    "    data = np.array(EEG[\"data\"])\n",
    "    srate = float(EEG[\"srate\"])\n",
    "\n",
    "    # Ensure (n_ch, n_samp)\n",
    "    nbchan = int(EEG.get(\"nbchan\", data.shape[0]))\n",
    "    if data.shape[0] != nbchan and data.shape[1] == nbchan:\n",
    "        data = data.T\n",
    "\n",
    "    # Channel names\n",
    "    ch_names = None\n",
    "    if ch_tsv.exists():\n",
    "        chdf = pd.read_csv(ch_tsv, sep=\"\\t\")\n",
    "        if \"name\" in chdf.columns and len(chdf[\"name\"]) == data.shape[0]:\n",
    "            ch_names = chdf[\"name\"].astype(str).tolist()\n",
    "\n",
    "    if ch_names is None:\n",
    "        chanlocs = EEG.get(\"chanlocs\", [])\n",
    "        if isinstance(chanlocs, list) and len(chanlocs) == data.shape[0]:\n",
    "            ch_names = [c.get(\"labels\", f\"ch{i}\") for i, c in enumerate(chanlocs)]\n",
    "        else:\n",
    "            ch_names = [f\"ch{i}\" for i in range(data.shape[0])]\n",
    "\n",
    "    # Scale heuristic: if values look like microvolts, convert to volts\n",
    "    peak = float(np.nanmax(np.abs(data)))\n",
    "    if peak > 1e3:\n",
    "        data = data * 1e-6\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=srate, ch_types=[\"eeg\"] * len(ch_names))\n",
    "    raw = mne.io.RawArray(data.astype(np.float64), info, verbose=False)\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503dbda8",
   "metadata": {},
   "source": [
    "## 2) Feature extraction: theta/alpha/beta bandpower per channel\n",
    "\n",
    "Feature vector length = `3 * n_channels`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c2266b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = {\n",
    "    \"theta\": (4, 8),\n",
    "    \"alpha\": (8, 13),\n",
    "    \"beta\":  (13, 30),\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "\n",
    "def bandpower_epoch(epoch, sf, fmin, fmax, nperseg=256):\n",
    "    \"\"\"\n",
    "    epoch: (n_channels, n_times) OR (n_times,)\n",
    "    returns: (n_channels,) bandpower\n",
    "    \"\"\"\n",
    "    epoch = np.asarray(epoch)\n",
    "\n",
    "    # Ensure shape is (n_channels, n_times)\n",
    "    if epoch.ndim == 1:\n",
    "        epoch = epoch[None, :]   # 1 channel\n",
    "    elif epoch.ndim == 3:\n",
    "        # if someone accidentally passes (n_epochs, n_channels, n_times),\n",
    "        # this function should not accept it\n",
    "        raise ValueError(f\"bandpower_epoch expected 1D or 2D, got {epoch.shape}\")\n",
    "\n",
    "    nperseg = min(nperseg, epoch.shape[-1])\n",
    "\n",
    "    freqs, psd = welch(epoch, fs=sf, axis=-1, nperseg=nperseg)  # psd: (n_channels, n_freqs)\n",
    "\n",
    "    idx = (freqs >= fmin) & (freqs <= fmax)\n",
    "    if not np.any(idx):\n",
    "        # no freqs in band: return zeros instead of crashing\n",
    "        return np.zeros(epoch.shape[0], dtype=np.float32)\n",
    "\n",
    "    bp = np.trapezoid(psd[:, idx], freqs[idx], axis=1)  # (n_channels,)\n",
    "    return bp.astype(np.float32)\n",
    "\n",
    "\n",
    "def make_feature_matrix(epochs: mne.Epochs) -> np.ndarray:\n",
    "    data = epochs.get_data()  # (n_epochs, n_ch, n_times)\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    feats = []\n",
    "    for ep in data:\n",
    "        parts = []\n",
    "        for (f1, f2) in BANDS.values():\n",
    "            parts.append(bandpower_epoch(ep, sf, f1, f2))\n",
    "        feats.append(np.concatenate(parts, axis=0))\n",
    "    return np.asarray(feats, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87884dbc",
   "metadata": {},
   "source": [
    "## 3) Build dataset across all runs (and cache)\n",
    "\n",
    "This step loads EEG, epochs trials, and computes features.\n",
    "A cache is saved to speed up reruns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e706b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded cache (parquet): /Users/tanmmay/Coding_projects/Neurotech_Model/notebooks/data/derived/ds004350_sart_features.parquet shape: (27704, 7)\n",
      "Using cached features: /Users/tanmmay/Coding_projects/Neurotech_Model/notebooks/data/derived/ds004350_sart_features.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "      <th>trial_idx</th>\n",
       "      <th>y</th>\n",
       "      <th>n_channels</th>\n",
       "      <th>sfreq</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-001</td>\n",
       "      <td>ses-post</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>256.0</td>\n",
       "      <td>[4.147689570271895e-12, 5.5804479960619435e-12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-001</td>\n",
       "      <td>ses-post</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>256.0</td>\n",
       "      <td>[5.5305612520206715e-12, 8.108990909305724e-12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-001</td>\n",
       "      <td>ses-post</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>256.0</td>\n",
       "      <td>[4.2431071677062615e-12, 5.568594630550594e-12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-001</td>\n",
       "      <td>ses-post</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>256.0</td>\n",
       "      <td>[2.105608439342821e-12, 2.199622211804253e-12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-001</td>\n",
       "      <td>ses-post</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>256.0</td>\n",
       "      <td>[3.6186600591014972e-12, 4.984446883016247e-12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject   session  trial_idx  y  n_channels  sfreq  \\\n",
       "0  sub-001  ses-post          0  0          64  256.0   \n",
       "1  sub-001  ses-post          1  0          64  256.0   \n",
       "2  sub-001  ses-post          2  0          64  256.0   \n",
       "3  sub-001  ses-post          3  0          64  256.0   \n",
       "4  sub-001  ses-post          4  0          64  256.0   \n",
       "\n",
       "                                            features  \n",
       "0  [4.147689570271895e-12, 5.5804479960619435e-12...  \n",
       "1  [5.5305612520206715e-12, 8.108990909305724e-12...  \n",
       "2  [4.2431071677062615e-12, 5.568594630550594e-12...  \n",
       "3  [2.105608439342821e-12, 2.199622211804253e-12,...  \n",
       "4  [3.6186600591014972e-12, 4.984446883016247e-12...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DERIVED_DIR = Path(\"data/derived\").resolve()\n",
    "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_PATH = DERIVED_DIR / \"ds004350_sart_features.parquet\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def _safe_load_cache(cache_path):\n",
    "    cache_path = Path(cache_path)\n",
    "\n",
    "    # 1) Try parquet\n",
    "    if cache_path.exists():\n",
    "        try:\n",
    "            df = pd.read_parquet(cache_path)\n",
    "            print(\"âœ… Loaded cache (parquet):\", cache_path, \"shape:\", df.shape)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ Failed to read parquet cache:\", repr(e))\n",
    "\n",
    "    # 2) Try pickle fallback\n",
    "    pkl_path = cache_path.with_suffix(\".pkl\")\n",
    "    if pkl_path.exists():\n",
    "        df = pd.read_pickle(pkl_path)\n",
    "        print(\"âœ… Loaded cache (pickle):\", pkl_path, \"shape:\", df.shape)\n",
    "        return df\n",
    "\n",
    "    # 3) Nothing found\n",
    "    return None\n",
    "\n",
    "\n",
    "def _safe_save_cache(df: pd.DataFrame, cache_path: Path) -> Path:\n",
    "    \"\"\"Save parquet cache safely; if pyarrow registry issues occur, fall back to pickle.\"\"\"\n",
    "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure features are plain Python lists (avoids Arrow extension type issues)\n",
    "    if \"features\" in df.columns:\n",
    "        df = df.copy()\n",
    "        df[\"features\"] = df[\"features\"].apply(lambda x: list(x) if not isinstance(x, list) else x)\n",
    "\n",
    "    # 1) Try normal pandas parquet\n",
    "    try:\n",
    "        df.to_parquet(cache_path, index=False)\n",
    "        return cache_path\n",
    "    except Exception as e1:\n",
    "        # 2) Try direct pyarrow write_table\n",
    "        try:\n",
    "            import pyarrow as pa\n",
    "            import pyarrow.parquet as pq\n",
    "            table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "            pq.write_table(table, str(cache_path))\n",
    "            return cache_path\n",
    "        except Exception as e2:\n",
    "            # 3) Fallback: pickle\n",
    "            pkl_path = cache_path.with_suffix(\".pkl\")\n",
    "            df.to_pickle(pkl_path)\n",
    "            print(\"Parquet save failed; saved pickle instead:\", pkl_path, \"| errors:\", repr(e1), repr(e2))\n",
    "            return pkl_path\n",
    "\n",
    "def eval_predictions(y_true, y_pred, y_prob=None) -> dict:\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, labels=[0, 1], zero_division=0\n",
    "    )\n",
    "    out = {\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"precision_no_lapse\": pr[0],\n",
    "        \"recall_no_lapse\": rc[0],\n",
    "        \"f1_no_lapse\": f1[0],\n",
    "        \"precision_lapse\": pr[1],\n",
    "        \"recall_lapse\": rc[1],\n",
    "        \"f1_lapse\": f1[1],\n",
    "    }\n",
    "    if y_prob is not None and len(np.unique(y_true)) == 2:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = roc_auc_score(y_true, y_prob)\n",
    "        except Exception:\n",
    "            out[\"roc_auc\"] = np.nan\n",
    "    else:\n",
    "        out[\"roc_auc\"] = np.nan\n",
    "    return out\n",
    "\n",
    "def build_or_load_features(\n",
    "    cache_path: Path = CACHE_PATH,\n",
    "    tmin: float = 0.0,\n",
    "    tmax: float = 1.5,\n",
    "    l_freq: float = 1.0,\n",
    "    h_freq: float = 40.0,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # ---------- Load cache safely ----------\n",
    "    df_cached = _safe_load_cache(cache_path)\n",
    "    if df_cached is not None and len(df_cached) > 0:\n",
    "        print(\"Using cached features:\", cache_path)\n",
    "        return df_cached\n",
    "\n",
    "    # ---------- Build ----------\n",
    "    rows = []\n",
    "    for ev_path in tqdm(run_events, desc=\"Runs\"):\n",
    "        sub, ses = parse_subject_session(ev_path)\n",
    "\n",
    "        ev = read_events_tsv(ev_path)\n",
    "        ev = label_sart_lapses(ev)  # produces ev[\"y\"]\n",
    "\n",
    "        # Load EEG\n",
    "        raw = load_eeg_from_set(ev_path)\n",
    "        raw.pick_types(eeg=True)\n",
    "\n",
    "        # Reference/filter (speed: allow multi-core if available)\n",
    "        raw.set_eeg_reference(\"average\", verbose=False)\n",
    "        raw.filter(l_freq, h_freq, n_jobs=-1, verbose=False)\n",
    "\n",
    "        sfreq = float(raw.info[\"sfreq\"])\n",
    "        onsets = ev[\"onset\"].astype(float).to_numpy()\n",
    "\n",
    "        # IMPORTANT: use y (not lapse)\n",
    "        y_all = ev[\"y\"].astype(int).to_numpy()\n",
    "\n",
    "        # Build MNE events array\n",
    "        samples = np.round(onsets * sfreq).astype(int)\n",
    "        samples = np.clip(samples, 0, raw.n_times - 1)\n",
    "\n",
    "        events = np.column_stack([\n",
    "            samples,\n",
    "            np.zeros(len(samples), dtype=int),\n",
    "            np.ones(len(samples), dtype=int),\n",
    "        ])\n",
    "\n",
    "        epochs = mne.Epochs(\n",
    "            raw,\n",
    "            events,\n",
    "            event_id={\"trial\": 1},\n",
    "            tmin=tmin,\n",
    "            tmax=tmax,\n",
    "            baseline=None,\n",
    "            preload=True,\n",
    "            reject_by_annotation=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # ---------- Align labels to kept epochs ----------\n",
    "        # epochs.selection are indices into the original \"events\" array\n",
    "        if len(epochs) == 0:\n",
    "            continue\n",
    "\n",
    "        kept = epochs.selection\n",
    "        y = y_all[kept]\n",
    "\n",
    "        # sanity\n",
    "        if len(y) != len(epochs):\n",
    "            raise RuntimeError(f\"Label/epoch mismatch after selection: y={len(y)} epochs={len(epochs)}\")\n",
    "\n",
    "        X = make_feature_matrix(epochs)\n",
    "        if X.shape[0] != len(y):\n",
    "            raise RuntimeError(f\"Feature/label mismatch: X={X.shape} y={len(y)}\")\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            rows.append({\n",
    "                \"subject\": sub,\n",
    "                \"session\": ses,\n",
    "                \"trial_idx\": int(i),\n",
    "                \"y\": int(y[i]),\n",
    "                \"n_channels\": int(epochs.info[\"nchan\"]),\n",
    "                \"sfreq\": float(epochs.info[\"sfreq\"]),\n",
    "                \"features\": X[i].tolist(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\n",
    "            \"No rows were created. Check that run_events is non-empty, epochs are being created, \"\n",
    "            \"and label_sart_lapses is producing y.\"\n",
    "        )\n",
    "\n",
    "    print(\"Built features:\", df.shape, \"overall lapse rate:\", float(df[\"y\"].mean()))\n",
    "    saved_path = _safe_save_cache(df, cache_path)\n",
    "    print(\"Saved cache:\", saved_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = build_or_load_features()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5cfe5",
   "metadata": {},
   "source": [
    "## 4) Prepare matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2a55292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-001_ses-post_task-SART_events.tsv pos: 25 / 575\n",
      "sub-001_ses-pre_task-SART_events.tsv pos: 40 / 560\n",
      "sub-002_ses-post_task-SART_events.tsv pos: 19 / 581\n",
      "sub-002_ses-pre_task-SART_events.tsv pos: 36 / 564\n",
      "sub-003_ses-post_task-SART_events.tsv pos: 25 / 575\n",
      "sub-003_ses-pre_task-SART_events.tsv pos: 19 / 581\n",
      "sub-004_ses-post_task-SART_events.tsv pos: 38 / 544\n",
      "sub-004_ses-pre_task-SART_events.tsv pos: 25 / 575\n",
      "sub-005_ses-post_task-SART_events.tsv pos: 30 / 570\n",
      "sub-005_ses-pre_task-SART_events.tsv pos: 32 / 568\n",
      "Total positives across 10 runs: 289 / 5693\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pos_total = 0\n",
    "n_total = 0\n",
    "\n",
    "for p in run_events[:10]:\n",
    "    ev = read_events_tsv(p)\n",
    "    ev = label_sart_lapses(ev)\n",
    "    pos = int(ev[\"y\"].sum()) if \"y\" in ev.columns else 0\n",
    "    pos_total += pos\n",
    "    n_total += len(ev)\n",
    "    print(p.name, \"pos:\", pos, \"/\", len(ev))\n",
    "\n",
    "print(\"Total positives across 10 runs:\", pos_total, \"/\", n_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b69f18e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (27704, 192) y: (27704,)\n",
      "Overall lapse rate: 0.04966791799018192\n",
      "Subjects: 24 Sessions: ['ses-post' 'ses-pre']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-024</th>\n",
       "      <td>1647</td>\n",
       "      <td>153</td>\n",
       "      <td>0.092896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-022</th>\n",
       "      <td>1132</td>\n",
       "      <td>68</td>\n",
       "      <td>0.060071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-014</th>\n",
       "      <td>1133</td>\n",
       "      <td>67</td>\n",
       "      <td>0.059135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-013</th>\n",
       "      <td>1134</td>\n",
       "      <td>66</td>\n",
       "      <td>0.058201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-001</th>\n",
       "      <td>1135</td>\n",
       "      <td>65</td>\n",
       "      <td>0.057269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-004</th>\n",
       "      <td>1119</td>\n",
       "      <td>63</td>\n",
       "      <td>0.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-010</th>\n",
       "      <td>1138</td>\n",
       "      <td>62</td>\n",
       "      <td>0.054482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-005</th>\n",
       "      <td>1138</td>\n",
       "      <td>62</td>\n",
       "      <td>0.054482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-008</th>\n",
       "      <td>1141</td>\n",
       "      <td>59</td>\n",
       "      <td>0.051709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-007</th>\n",
       "      <td>1142</td>\n",
       "      <td>58</td>\n",
       "      <td>0.050788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count  sum      mean\n",
       "subject                      \n",
       "sub-024   1647  153  0.092896\n",
       "sub-022   1132   68  0.060071\n",
       "sub-014   1133   67  0.059135\n",
       "sub-013   1134   66  0.058201\n",
       "sub-001   1135   65  0.057269\n",
       "sub-004   1119   63  0.056300\n",
       "sub-010   1138   62  0.054482\n",
       "sub-005   1138   62  0.054482\n",
       "sub-008   1141   59  0.051709\n",
       "sub-007   1142   58  0.050788"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.vstack(df[\"features\"].apply(np.array).values)\n",
    "y = df[\"y\"].astype(int).values\n",
    "groups = df[\"subject\"].values\n",
    "\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)\n",
    "print(\"Overall lapse rate:\", y.mean())\n",
    "print(\"Subjects:\", df[\"subject\"].nunique(), \"Sessions:\", df[\"session\"].unique())\n",
    "\n",
    "per_sub = df.groupby(\"subject\")[\"y\"].agg([\"count\",\"sum\",\"mean\"]).sort_values(\"mean\", ascending=False)\n",
    "display(per_sub.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687a356",
   "metadata": {},
   "source": [
    "## 5) Define models (LDA / RF / SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52e7ca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  XGBoost will use CPU (GPU not detected)\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"LDA\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LinearDiscriminantAnalysis())\n",
    "    ]),\n",
    "    \"RF\": RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        max_depth=None\n",
    "    ),\n",
    "    \"SVM-RBF\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\", C=2.0, gamma=\"scale\"))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "# Optional: GPU-accelerated XGBoost baseline (much faster than RF/SVM on large feature sets)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    def _xgb_use_gpu() -> bool:\n",
    "        # Works on Colab; safe fallback on local machines\n",
    "        try:\n",
    "            import subprocess, shlex\n",
    "            out = subprocess.check_output(shlex.split(\"nvidia-smi -L\"), stderr=subprocess.STDOUT).decode()\n",
    "            return \"GPU\" in out\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    use_gpu = _xgb_use_gpu()\n",
    "    xgb_kwargs = dict(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"aucpr\",\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        min_child_weight=1.0,\n",
    "        gamma=0.0,\n",
    "        random_state=42,\n",
    "    )\n",
    "    if use_gpu:\n",
    "        xgb_kwargs.update(dict(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\", device=\"cuda\"))\n",
    "        print(\"ðŸš€ XGBoost will use GPU\")\n",
    "    else:\n",
    "        xgb_kwargs.update(dict(tree_method=\"hist\"))\n",
    "        print(\"ðŸ§  XGBoost will use CPU (GPU not detected)\")\n",
    "\n",
    "    models[\"XGB\"] = XGBClassifier(**xgb_kwargs)\n",
    "except Exception as e:\n",
    "    print(\"XGBoost not available; skipping XGB model:\", repr(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc42daf0",
   "metadata": {},
   "source": [
    "## 6) Leave-One-Subject-Out evaluation (subject generalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a82d1909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials: 22255 Test trials: 5449\n",
      "Train lapse rate: 0.050640305549314764 Test lapse rate: 0.04569645806570013\n",
      "Train subjects: 19 Test subjects: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_no_lapse</th>\n",
       "      <th>recall_no_lapse</th>\n",
       "      <th>f1_no_lapse</th>\n",
       "      <th>precision_lapse</th>\n",
       "      <th>recall_lapse</th>\n",
       "      <th>f1_lapse</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>model</th>\n",
       "      <th>n_test</th>\n",
       "      <th>test_lapse_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.954304</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.954304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717149</td>\n",
       "      <td>XGB</td>\n",
       "      <td>5449</td>\n",
       "      <td>0.045696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.953019</td>\n",
       "      <td>0.499327</td>\n",
       "      <td>0.954245</td>\n",
       "      <td>0.998654</td>\n",
       "      <td>0.975944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674992</td>\n",
       "      <td>LDA</td>\n",
       "      <td>5449</td>\n",
       "      <td>0.045696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  balanced_accuracy  precision_no_lapse  recall_no_lapse  \\\n",
       "1  0.954304           0.500000            0.954304         1.000000   \n",
       "0  0.953019           0.499327            0.954245         0.998654   \n",
       "\n",
       "   f1_no_lapse  precision_lapse  recall_lapse  f1_lapse   roc_auc model  \\\n",
       "1     0.976618              0.0           0.0       0.0  0.717149   XGB   \n",
       "0     0.975944              0.0           0.0       0.0  0.674992   LDA   \n",
       "\n",
       "   n_test  test_lapse_rate  \n",
       "1    5449         0.045696  \n",
       "0    5449         0.045696  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FAST SUBJECT-WISE EVAL (quick results)\n",
    "# - Uses GroupShuffleSplit (hold out whole subjects once) instead of full CV\n",
    "# - Trains only a couple fast models first\n",
    "# - Prints metrics immediately\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Build X, y, groups if you haven't already\n",
    "# X = np.vstack(df[\"features\"].values).astype(np.float32)\n",
    "# y = df[\"y\"].astype(int).to_numpy()\n",
    "# groups = df[\"subject\"].to_numpy()\n",
    "\n",
    "# 1) One subject-wise split (fast)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "tr, te = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "Xtr, Xte = X[tr], X[te]\n",
    "ytr, yte = y[tr], y[te]\n",
    "\n",
    "print(\"Train trials:\", len(tr), \"Test trials:\", len(te))\n",
    "print(\"Train lapse rate:\", float(ytr.mean()), \"Test lapse rate:\", float(yte.mean()))\n",
    "print(\"Train subjects:\", len(np.unique(groups[tr])), \"Test subjects:\", len(np.unique(groups[te])))\n",
    "\n",
    "# 2) Pick only fast models for a quick sanity check\n",
    "# (Adjust names if your dict uses different keys)\n",
    "fast_model_names = [n for n in [\"LDA\", \"LogReg\", \"SGD\", \"XGB\"] if n in models]\n",
    "if not fast_model_names:\n",
    "    # fallback: just take first 2 models\n",
    "    fast_model_names = list(models.keys())[:2]\n",
    "\n",
    "quick_results = []\n",
    "for name in fast_model_names:\n",
    "    m = clone(models[name])\n",
    "    m.fit(Xtr, ytr)\n",
    "\n",
    "    yhat = m.predict(Xte)\n",
    "\n",
    "    prob = None\n",
    "    if hasattr(m, \"predict_proba\"):\n",
    "        prob = m.predict_proba(Xte)[:, 1]\n",
    "    elif hasattr(m, \"decision_function\"):\n",
    "        scores = m.decision_function(Xte)\n",
    "        prob = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
    "\n",
    "    agg = eval_predictions(yte, yhat, prob)\n",
    "    agg[\"model\"] = name\n",
    "    agg[\"n_test\"] = int(len(yte))\n",
    "    agg[\"test_lapse_rate\"] = float(yte.mean())\n",
    "    quick_results.append(agg)\n",
    "\n",
    "res_fast = pd.DataFrame(quick_results).sort_values(\"balanced_accuracy\", ascending=False)\n",
    "display(res_fast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54521f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2237a997",
   "metadata": {},
   "source": [
    "## 7) Cross-session (within subject): train `ses-pre` â†’ test `ses-post`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "954b41a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_no_lapse</th>\n",
       "      <th>recall_no_lapse</th>\n",
       "      <th>f1_no_lapse</th>\n",
       "      <th>precision_lapse</th>\n",
       "      <th>recall_lapse</th>\n",
       "      <th>f1_lapse</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>n_subjects_used</th>\n",
       "      <th>n_trials</th>\n",
       "      <th>lapse_rate</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.766468</td>\n",
       "      <td>0.542361</td>\n",
       "      <td>0.954302</td>\n",
       "      <td>0.791865</td>\n",
       "      <td>0.865528</td>\n",
       "      <td>0.070157</td>\n",
       "      <td>0.292857</td>\n",
       "      <td>0.113197</td>\n",
       "      <td>0.582119</td>\n",
       "      <td>24</td>\n",
       "      <td>13754</td>\n",
       "      <td>0.050894</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.891813</td>\n",
       "      <td>0.527952</td>\n",
       "      <td>0.952009</td>\n",
       "      <td>0.933047</td>\n",
       "      <td>0.942433</td>\n",
       "      <td>0.089583</td>\n",
       "      <td>0.122857</td>\n",
       "      <td>0.103614</td>\n",
       "      <td>0.620672</td>\n",
       "      <td>24</td>\n",
       "      <td>13754</td>\n",
       "      <td>0.050894</td>\n",
       "      <td>SVM-RBF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.947579</td>\n",
       "      <td>0.517447</td>\n",
       "      <td>0.950800</td>\n",
       "      <td>0.996323</td>\n",
       "      <td>0.973030</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.038571</td>\n",
       "      <td>0.069677</td>\n",
       "      <td>0.670900</td>\n",
       "      <td>24</td>\n",
       "      <td>13754</td>\n",
       "      <td>0.050894</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.755853</td>\n",
       "      <td>0.491478</td>\n",
       "      <td>0.948059</td>\n",
       "      <td>0.785813</td>\n",
       "      <td>0.859345</td>\n",
       "      <td>0.047035</td>\n",
       "      <td>0.197143</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.447126</td>\n",
       "      <td>24</td>\n",
       "      <td>13754</td>\n",
       "      <td>0.050894</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  balanced_accuracy  precision_no_lapse  recall_no_lapse  \\\n",
       "0  0.766468           0.542361            0.954302         0.791865   \n",
       "2  0.891813           0.527952            0.952009         0.933047   \n",
       "3  0.947579           0.517447            0.950800         0.996323   \n",
       "1  0.755853           0.491478            0.948059         0.785813   \n",
       "\n",
       "   f1_no_lapse  precision_lapse  recall_lapse  f1_lapse   roc_auc  \\\n",
       "0     0.865528         0.070157      0.292857  0.113197  0.582119   \n",
       "2     0.942433         0.089583      0.122857  0.103614  0.620672   \n",
       "3     0.973030         0.360000      0.038571  0.069677  0.670900   \n",
       "1     0.859345         0.047035      0.197143  0.075949  0.447126   \n",
       "\n",
       "   n_subjects_used  n_trials  lapse_rate    model  \n",
       "0               24     13754    0.050894      LDA  \n",
       "2               24     13754    0.050894  SVM-RBF  \n",
       "3               24     13754    0.050894      XGB  \n",
       "1               24     13754    0.050894       RF  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_session_eval(df: pd.DataFrame, model) -> dict:\n",
    "    subs = sorted(df[\"subject\"].unique())\n",
    "    all_true, all_pred, all_prob = [], [], []\n",
    "    kept = 0\n",
    "\n",
    "    for sub in subs:\n",
    "        dsub = df[df[\"subject\"] == sub]\n",
    "        if not ((\"ses-pre\" in set(dsub[\"session\"])) and (\"ses-post\" in set(dsub[\"session\"]))):\n",
    "            continue\n",
    "\n",
    "        pre = dsub[dsub[\"session\"] == \"ses-pre\"]\n",
    "        post = dsub[dsub[\"session\"] == \"ses-post\"]\n",
    "\n",
    "        Xpre = np.vstack(pre[\"features\"].apply(np.array).values)\n",
    "        ypre = pre[\"y\"].astype(int).values\n",
    "        Xpost = np.vstack(post[\"features\"].apply(np.array).values)\n",
    "        ypost = post[\"y\"].astype(int).values\n",
    "\n",
    "        # Need both classes in train/test for meaningful metrics\n",
    "        if len(np.unique(ypre)) < 2 or len(np.unique(ypost)) < 2:\n",
    "            continue\n",
    "\n",
    "        model.fit(Xpre, ypre)\n",
    "        yhat = model.predict(Xpost)\n",
    "\n",
    "        prob = None\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            prob = model.predict_proba(Xpost)[:, 1]\n",
    "        elif hasattr(model, \"decision_function\"):\n",
    "            scores = model.decision_function(Xpost)\n",
    "            prob = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
    "\n",
    "        all_true.append(ypost)\n",
    "        all_pred.append(yhat)\n",
    "        if prob is not None:\n",
    "            all_prob.append(prob)\n",
    "\n",
    "        kept += 1\n",
    "\n",
    "    if kept == 0:\n",
    "        return {\"error\": \"No subjects had both sessions with lapses in both train and test.\"}\n",
    "\n",
    "    y_true = np.concatenate(all_true)\n",
    "    y_pred = np.concatenate(all_pred)\n",
    "    y_prob = np.concatenate(all_prob) if len(all_prob) else None\n",
    "    out = eval_predictions(y_true, y_pred, y_prob)\n",
    "    out[\"n_subjects_used\"] = int(kept)\n",
    "    out[\"n_trials\"] = int(len(y_true))\n",
    "    out[\"lapse_rate\"] = float(y_true.mean())\n",
    "    return out\n",
    "\n",
    "cross_rows = []\n",
    "for name, model in models.items():\n",
    "    out = cross_session_eval(df, model)\n",
    "    out[\"model\"] = name\n",
    "    cross_rows.append(out)\n",
    "\n",
    "cross_df = pd.DataFrame(cross_rows).sort_values(\"balanced_accuracy\", ascending=False)\n",
    "display(cross_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab981e10",
   "metadata": {},
   "source": [
    "## 8) Optional: Inspect one subjectâ€™s confusion matrix quickly\n",
    "\n",
    "Pick a subject and run preâ†’post for that subject only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best-performing model (by balanced accuracy) after running the CV cell above\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# res_df is created in the CV cell; pick best model name\n",
    "best_name = res_df.iloc[0][\"model\"]\n",
    "best_model = models[best_name]\n",
    "\n",
    "# Fit on all data\n",
    "best_model.fit(X, y)\n",
    "\n",
    "OUT_DIR = Path(\"data/derived\").resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model_path = OUT_DIR / f\"best_model_{best_name}.joblib\"\n",
    "joblib.dump(best_model, model_path)\n",
    "print(\"âœ… Saved best model:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72aa08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def subject_pre_post(df, subject: str, model):\n",
    "    dsub = df[df[\"subject\"] == subject]\n",
    "    pre = dsub[dsub[\"session\"] == \"ses-pre\"]\n",
    "    post = dsub[dsub[\"session\"] == \"ses-post\"]\n",
    "    if pre.empty or post.empty:\n",
    "        raise ValueError(\"Subject missing pre or post session.\")\n",
    "\n",
    "    Xpre = np.vstack(pre[\"features\"].apply(np.array).values)\n",
    "    ypre = pre[\"y\"].astype(int).values\n",
    "    Xpost = np.vstack(post[\"features\"].apply(np.array).values)\n",
    "    ypost = post[\"y\"].astype(int).values\n",
    "\n",
    "    model.fit(Xpre, ypre)\n",
    "    yhat = model.predict(Xpost)\n",
    "\n",
    "    print(f\"{subject} preâ†’post balanced acc:\", balanced_accuracy_score(ypost, yhat))\n",
    "    cm = confusion_matrix(ypost, yhat, labels=[0,1])\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"no-lapse\",\"lapse\"])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "# subject_pre_post(df, \"sub-001\", models[\"SVM-RBF\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
